{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bec684-0fb0-48f2-818f-a77272a298e0",
   "metadata": {},
   "source": [
    "# PSY 341K Text Analysis for Behavioral Data Science\n",
    "##### Spring 2024; written by: Prof Desmond Ong (desmond.ong@utexas.edu)\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "In the tutorial we learnt how to train (from scratch) a Recurrent Neural Network (RNN) to classify text.\n",
    "\n",
    "In this assignment we'll be using a pre-trained contextual word embedding model (BERT), and we will be fine-tuning it on a sentiment classification example (see the slides for a high-level illustration of what this means.). This is quite \"standard\" in text analysis nowadays, and many projects adopt this approach.\n",
    "\n",
    "\n",
    "There are a lot of libraries out there to help make this easy. We will be using the `transformers` library provided by ðŸ¤— `huggingface`, which provides a lot of functionality. Much of this code is taken from the `huggingface transformers` documentation/tutorials, including the [quicktour](https://huggingface.co/docs/transformers/main/en/quicktour), and the [preprocessing](https://huggingface.co/docs/transformers/main/en/preprocessing) and [fine-tuning](https://huggingface.co/docs/transformers/main/en/training) tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313b44fa-eb41-484c-a40d-269e796e0f65",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: datasets in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: evaluate in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (2023.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: packaging in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from evaluate) (0.13.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from responses<0.19->evaluate) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Collecting xformers\n",
      "  Using cached xformers-0.0.25.post1.tar.gz (4.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.1 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from xformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from xformers) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from torch>=2.1->xformers) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from torch>=2.1->xformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from torch>=2.1->xformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from torch>=2.1->xformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from torch>=2.1->xformers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from torch>=2.1->xformers) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2.1->xformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ruthcarter/anaconda3/lib/python3.11/site-packages (from sympy->torch>=2.1->xformers) (1.3.0)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[354 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_deprecation_warning.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/attn_bias_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/checkpoint.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/fused_linear_layer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_fw.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/dropout.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_dropout.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_bw.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/simplicial_embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/reversible.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/patch_embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_blocksparse_transformers.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attn_decoder.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_transformer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_layernorm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_causal_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_fused_linear.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention_mqa.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_dropout.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sp24.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rmsnorm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/modpar_layers.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rope_padded.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/seqpar.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/ipc.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/sp24.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/differentiable_collectives.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/tiled_matmul.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler_dcgm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/slow_ops_profiler.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/test_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/hierarchical_configs.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/timm_sparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/hydra_helper.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_factory.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/model_factory.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_configs.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/weight_init.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/global_tokens.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/ortho.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/local.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/compositional.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/pooling.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/lambda_layer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/random.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/linformer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/visual.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/nystrom.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/favor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/conv_mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/fused_mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/vocab.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/param.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/sine.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/sequence_parallel_fused_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/decoder.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck_decoder.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/small_k.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bigcode.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/btlm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/baichuan.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sequence_parallel_fused\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/swiglu\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m [1/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd/matmul.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/autograd/matmul.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd/matmul.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd/matmul.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/autograd/matmul.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [2/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/matmul.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/matmul.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/matmul.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/matmul.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/matmul.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [3/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/sparse_softmax.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/sparse_softmax.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [4/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sddmm.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/sddmm.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sddmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sddmm.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sddmm.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/sddmm.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sddmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [5/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/spmm.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/spmm.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/spmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/spmm.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/spmm.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/spmm.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/spmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [6/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sddmm.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/sddmm.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sddmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sddmm.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sddmm.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/sddmm.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/sddmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [7/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/attention.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/attention.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [8/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/matmul.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/matmul.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/matmul.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/matmul.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/cpu/matmul.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [9/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24/sparse24.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/sparse24/sparse24.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24/sparse24.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24/sparse24.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24/sparse24.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/sparse24/sparse24.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24/sparse24.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m [10/16] c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sparse_softmax.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/sparse_softmax.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sparse_softmax.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0m/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sparse_softmax.o\n",
      "  \u001b[31m   \u001b[0m c++ -MMD -MF /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sparse_softmax.o.d -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ruthcarter/anaconda3/include -arch arm64 -I/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ruthcarter/anaconda3/include/python3.11 -c -c /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/xformers/csrc/attention/sparse_softmax.cpp -o /private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/sparse_softmax.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m ninja: build stopped: subcommand failed.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2096, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/subprocess.py\", line 571, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/setup.py\", line 483, in <module>\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/__init__.py\", line 107, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 325, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 84, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/c2/vqcf1d1j7k76wnxgnj0h0j680000gn/T/pip-install-nn0bxday/xformers_79d5b2ce4290407abcb31c3795fe239e/setup.py\", line 440, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     super().build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 871, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 467, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 493, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 246, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 548, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 684, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1774, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/ruthcarter/anaconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2112, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for xformers\n",
      "Failed to build xformers\n",
      "\u001b[31mERROR: Could not build wheels for xformers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install datasets\n",
    "! pip install evaluate\n",
    "! pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdd46f-b9a8-448f-85c5-a5ad3ade1c1d",
   "metadata": {},
   "source": [
    "The `transformers` library from ðŸ¤— `huggingface` has an \"all-in-one\" command that can load several pre-trained models for certain tasks, using `pipeline`. So for example, if you wanted to just get some sentiment analysis done, you could call:\n",
    "\n",
    "```pipeline(task=\"sentiment-analysis\")```\n",
    "\n",
    "and it will load its default sentiment analysis model for you. You could also specify one of many different models that it already has. (Or different tasks). Then you can just pass it any text you want. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ea2afb-e3bf-473e-bbd1-ba6008ef9be5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"sentiment-analysis\")\n",
    "\n",
    "classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e265c5-6941-4a16-a2b7-6f6a9666c410",
   "metadata": {},
   "source": [
    "Some models are really powerful too. For example, we talked about `zero-shot-classification` in class. Here's an example from the [pipeline tutorial](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial), where you can import a pre-trained model that can try to infer which of a set of candidate labels apply to the input text. It is zero-shot in the sense that you can give it any new label you require, and it does not have to be trained on the label.\n",
    "\n",
    "(Note these models take up some space, so I've left it in comments so you can run it if you're interested)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7643d78-d9bf-4d6a-89b3-9419f2323241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## This model is a `zero-shot-classification` model.\n",
    "# ## It will classify text, except you are free to choose any label you might imagine\n",
    "#classifier = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "#classifier(\"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "#candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5eaf63-850a-44ba-819b-0b2fbbd74b2d",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained BERT model\n",
    "\n",
    "In this assignment we want to fine-tune a pre-trained BERT model on another dataset. After this assignment you can apply this same approach to fine-tuning models to your particular dataset of interest.\n",
    "\n",
    "Recall the basic deep learning pipeline, which is 1) Set Up Data, 2) Define Model, 3) Set up Training Parameters, 4) Run and 5) Evaluate the model.\n",
    "\n",
    "The `huggingface` `transformers` library already does lots of things for us. Let's use the `bert-base-uncased` model, which is the \"base\" BERT model trained on uncased (i.e., all-lower-cased) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63f7b5c-51c5-478b-964d-219572a26b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb1aea-d2e6-44e0-a0d2-fe55399911cf",
   "metadata": {},
   "source": [
    "## Set up Data\n",
    "\n",
    "We can use the `AutoTokenzier` function from the transformers library, which provides the corresponding tokenizer to your desired model.\n",
    "\n",
    "The important thing to note is to use the same pre-trained tokenizer that corresponds to the model that you are using. Different models are pre-trained with different tokenizing rules (e.g., subwords). Also, `bert-base-uncased`, as its name suggests, is trained on uncased data---Part of what the tokenizer does is to apply some of these transformations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfec3534-6802-4a52-b674-e1f08b45f2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3034831-dc00-4891-a22f-8b389e2fcf20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are some example code on how to use the tokenizer.\n",
    "# note that the tokenizer adds in a [CLS] token at the start, and a [SEP] token at the end.\n",
    "# these are just to mark the start and end of the input, and are what the BERT models expect.\n",
    "\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc64c5b-f74e-4828-9c19-5763f79a38d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "          1010, 28315,  1012,   102],\n",
      "        [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# and for batches of sentences:\n",
    "# we can see that the encoded inputs are tensors\n",
    "# the input_ids are the id numbers of the words in the vocabulary. \n",
    "#    For example, \"but\" is word ID 2021      (CLS is 101)\n",
    "# the options:\n",
    "#    we pad short sentences and truncate too-long ones (limit determined by specific model)\n",
    "#    and we ask it to return tensors that `pytorch` (pt) expects. \n",
    "#       (huggingface works with other frameworks besides pytorch too)\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b23da-b4e9-4b3f-8deb-12bf61d9bd17",
   "metadata": {},
   "source": [
    "For datasets, we'll also use one of the existing `dataset`s that is prepared by `huggingface`.\n",
    "\n",
    "We'll use a dataset of Yelp Reviews, which is a **5-class** sentiment classification task (1-star to 5-stars). \n",
    "\n",
    "\n",
    "We'll also use just 300 of those reviews for training (just to get a sense of how it's done; and not to kill your laptops.) \n",
    "\n",
    "- The full dataset is 650,000 in the training set and 50,000 in the evaluation set.\n",
    "- With 300 training examples, each epoch of training takes about 7 mins on the instructor's laptop.\n",
    "- With 1000 training examples, each epoch of training takes about 45 mins on the instructor's laptop.\n",
    "- Of course, with more training examples and more training epochs, the model's performance will increase.\n",
    "\n",
    "Normally deep learning projects should be run on a cluster rather than on your own computer, but here we'll keep it simple (and small!) so you can learn the basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb59cd67-402f-453b-9daa-e3d0b172d456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/ruthcarter/.cache/huggingface/datasets/parquet/yelp_review_full-9c7006f5a2e02666/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ac40bd29ab40c98d8a78ca56df91d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/ruthcarter/.cache/huggingface/datasets/parquet/yelp_review_full-9c7006f5a2e02666/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e585686224d0228d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# These steps are necessary to convert the dataset into something that pytorch expects\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e28050-00d6-4d64-bb10-51b532f1ea82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/ruthcarter/.cache/huggingface/datasets/parquet/yelp_review_full-9c7006f5a2e02666/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2e4d55fe882549ff.arrow\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SET_SIZE = 300\n",
    "\n",
    "# creating a small, 300 example dataset to train and evaluate our model\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(SAMPLE_SET_SIZE))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(SAMPLE_SET_SIZE))\n",
    "\n",
    "\n",
    "# using DataLoader to prep the training and evaluation datasets into something torch expects\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc475475-3a04-4cb5-8854-c249a2daf5d0",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Next, we define our model, which we want to be a Sequence Classification model built on top of a `bert-base-uncased` pre-trained model. The `AutoModelForSequenceClassification` function from the `transformers` library allows us to easily specify this according to several default choices (e.g., number of hidden units, etc. These can be modified).\n",
    "\n",
    "Note that we are defining a model with **5 labels**. (This is because the Yelp Review dataset we are using has labels of 1-5 stars.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc785b3-3450-49e6-8199-52f94ba9dd95",
   "metadata": {},
   "source": [
    "Note, the warning that shows up just says that some of the weights were not initialized, which is the `cls.predictions` layer (i.e., the output prediction layer that is newly-initialized and trained). Note that it says \"This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d91b4d87-96a3-4e73-8d13-d408216c038a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2420b-9ba9-4b40-bb02-9a4caa3171f9",
   "metadata": {},
   "source": [
    "## Set up training / fine-tuning parameters\n",
    "\n",
    "Just like the tutorial, we define our optimizer, the learning rate, the number of epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fd54a22-64d6-414d-97c2-4e3c90886b37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# if you have a GPU it'll send the code to the GPU, otherwise it'll use the CPU.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5081a-9743-4c64-b613-dc3cd63df6da",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Then we run our training loop! The progress bar will update regularly with the time-taken so far and allow you to estimate the time remaining.\n",
    "\n",
    "(The code above, by default, will train the model for 2 epochs. On the instructor's laptop, it takes ~7 mins per epoch. You may choose to run your training for longer, to get better results.)\n",
    "\n",
    "The evaluation step also takes a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e19513f-1fc9-4531-b94d-2f8692579dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe0e08f8aad41b2805f4f292375b041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# providing a progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train() # switch the model to \"training mode\". \n",
    "# one of the reasons is that this activates dropout during training, \n",
    "# which is a regularization technique that helps the model avoid overfitting.\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "153f7274-c42e-4d54-9561-e25aee809698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval() # switch the model to \"evaluation mode\". \n",
    "# this deactivates dropout during evaluation.\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f955155-dbf7-48c7-840e-a42bbbe4b5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2818,  0.0533, -0.4202, -0.6646, -0.7030],\n",
       "        [-1.1152, -0.0643,  0.1303,  0.5088,  0.3549],\n",
       "        [-0.6903,  0.1139, -0.0249,  0.5008,  0.3043],\n",
       "        [-1.2655, -0.3152, -0.0968,  0.6727,  0.6639]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87053d2-6628-4d44-855f-1df81fb8de61",
   "metadata": {},
   "source": [
    "Is the accuracy good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c54824-e038-4989-a376-f2788e72d723",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60f9ed-fc56-4ac7-a56f-a18c40173df7",
   "metadata": {},
   "source": [
    "- An accuracy of 48.33% does not seem that impressive. More than half of the time, the model predicts incorrectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230a5e1-6c37-4757-b220-7ed3e5bf73c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manually evaluating the model\n",
    "\n",
    "Ok now that we have a fine-tuned model, let's see how well it does. This model is fine-tuned on Yelp reviews. Can you pull a couple of product reviews (from the web; it does not have to be Yelp, please try some websites that you may be familiar with), or other types of text, feed them into the model, and then see what the output says? \n",
    "\n",
    "Try at least ten different reviews or texts, and include a short discussion of each. For example, some questions that you could  \n",
    "\n",
    "- Does the model do well? \n",
    "- Is the model able to capture specific characteristics of each piece of text? \n",
    "- Do you agree with the model? If yes, why? If no, why not?\n",
    "\n",
    "(You are not limited to these questions above, these are just examples. Try to ask your *own* questions about this model and the data!)\n",
    "\n",
    "\n",
    "- Hint: try to get a variety of complex examples to test the model on. For example, we talked about aspect-based sentiment analysis in class (although this particular model we've trained isn't trained to separate different aspects). If you feed it a complex review with several different \"parts\" (that say, talk about different aspects), what does the model predict? Does it agree with your own intuitions?\n",
    "- What abour sarcasm? Can you find (or write) reviews that could be read as sarcastic by a human? Does the model think so too?\n",
    "- What about other TYPES of reviews? Can the model generalize?\n",
    "- What about other types of text? Can the model generalize?\n",
    "\n",
    "Be creative and try to see how well this model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094e7a0-129a-476f-963f-2d53b38f12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_review = \"UT is the best school ever\"\n",
    "\n",
    "## Task: \n",
    "## now that you have a trained model, \n",
    "## write some code to feed some hand-picked examples into the model, and get the output back out.\n",
    "## see if the output class \"agrees\" with what you might expect.\n",
    "## be sure to include a short discussion for each example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Hint1: there is code in the assignment above to pass in example sentences.\n",
    "##     you'll need to \"encode\" the sentences (convert words into word vectors/embeddings)\n",
    "\n",
    "## Hint2 then you'll need to call the model() function on the input. Something like model(**input). \n",
    "##    (The model had to process the sentences in both the training and the evaluation steps... \n",
    "##    Which part of the code contains the single line that you need?\n",
    "\n",
    "## Hint3: After getting the output of the model (... output = model(**input) ...)\n",
    "##     you can get the logits using output.logits. For one example, this will give something like\n",
    "##         tensor([ (logit of class 1), (logit of class 2), (logit of class 3), ... ])\n",
    "##     i.e., the model is predicting the log-odds of each class. \n",
    "##     We can do something as simple as taking the class with the largest logit to be the label for that example\n",
    "##    (There is a \"argmax\" function in the torch library which will be useful)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff939af6-63eb-4e8d-a61d-d91e2e0c4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hint1: there is code in the assignment above to pass in example sentences.\n",
    "##     you'll need to \"encode\" the sentences (convert words into word vectors/embeddings)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# These steps are necessary to convert the dataset into something that pytorch expects\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f835e-0c6f-43e9-995c-d415e8ad6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33152922-6464-4825-927d-d957ec0aa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_reviews = [\n",
    "    \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120304c-74e3-4aba-bd31-2c243aa70afd",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f871a758-f18c-4188-a97d-0ec32cdbaca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world! Howdy.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1563\u001b[0m     input_ids,\n\u001b[1;32m   1564\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1565\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1566\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1567\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1568\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1569\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1570\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1571\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1572\u001b[0m )\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:968\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model(**tokenizer(\"Hello, world! Howdy.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d4a2c-aa32-4a1a-913e-2472748cc4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
